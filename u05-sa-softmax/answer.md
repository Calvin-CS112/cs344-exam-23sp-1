Softmax divides by x.exp().sum() to normalize the values, so they're all on the same scale and comparable to one another. Otherwise, the loss for an incorrect prediction could be lower than for a correct prediction.